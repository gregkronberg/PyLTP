''' process two-pathway ltp experiments recorded with LabChart software

==Experiment Design==
-each synpatic pathway in the slice is activated in an alternating fashion every 30 s with each baseline block recorded for 1 s.  The pulse from the bipolar electrode occurs at 0.5 s after the start of the block.  The activated pathway for each block is marked by a comment at the start of the block; e.g. 'path1' or 'path2'

-at the start of each experiment an i/o curve is generated by increasing the stimulation intensity for each pathway until the fEPSP slope changes by less than 10%.  the maximum intensity is marked in most experiments by a comment of the form 'baseline_max_350', where the number is the stimulus current required to achieve the max.  bipolar stimulus intensity is then set to achieve 40% of this maximum slope, which is marked by a comment of the form 'baseline_40percent_100'.  most experiments also contain comments to specify the location of the stimulatiing electrode for each pathway by labeling as either 'orthodromic' or 'antidromic'.

-when baseline responses are stable in both pathways (<5% drift over 30 minutes), plasticity is induced

plasticity induction blocks
``````````````````````````
-induction comments
    -'TBS': 4 pulses at 100 Hz, repeated 15 times at 5 Hz
    -'weak5Hz' : 15 pulses at 5 hz
        -in one slice, this condition is called :'15pulse5Hz'
        -prior to 20180614 these pulses were timed to be between the 2nd and 3rd pulse of TBS 15 ms after the onset of each burst
        -after 20180614 weak5Hz pulses are timed to the second pulse of each burst, i.e. 10 ms after burst onset
    -'nostim': pathway is inactive during induction
    -each induction block should contain a comment that specifies the induction protocol for each pathway in the form:
            -'induction_path1_TBS_anodal_20Vm_apical', indicating the first induction for pathway 1 was a theta burst (4 pulses @ 100 Hz repeated 15 times at 5 Hz) during anodal DCS at 20 Vm in apical dendrites 
            -another example:'induction2_path2_weak5Hz_control_0Vm_apical'

    -a comment is also made on each recording channel to specify the location of that electrode, e.g. the 'soma' on channel 1 and 'apical' on channel 2

    -'field_250' specifies the amount of current applied to achieve the desired electric field magnitude.  If this comment is during a control induction (no field) it specifies the amount of current required for a 20 V/m field

    -'isolator_off' indicates that the field isolator is off during the experiment and only turned on during the induction period when it is needed

important dates
`````````````````````````````````
-20180613
    -for experiments after 20180613 a function generator was set up to manually apply DCS during induction (labchart could not control 3 outputs simultaneously).  after this date, induction periods were changed from 7 to 15 s and the duration of DCS from 5 to 8 s.  This was to allow for some manual error in the timing of DCS while still capturing the full induction during DCS
    -prior to 20180613: induction duration was 7 s with DCS onset at 1 s for 5 s, TBS onset at 2 s for 3 s.
    -after 20180613: induction during was 15 s.  DCS onset was variable but attempted to be at 3 s for 8 s with induction onset at 6 s for 3 s. 

-20180614
    -the timing of 'weak5Hz' stimulation was changed to be aligned with the second pulse of each burst in a theta burst (10ms after burst onset).  prior to this date, weak5Hz is aligned to be between the second and third pulse (15 ms after burst onset)

-20180920
    -began doing my own dissection instead of using zeinab's slices

-20181113
    -ended pilot collection for paired (TBS + weak 5Hz condition) at lower bipolar stimulus intensity (30% of max instead of 40%)

-20181212
    -began running TBS unpaired experiments with 30% baseline fEPSPs

-20181217 : began using 100 uA pulses to calibrate field instead of 10 uA pulses

-20180115 : finished TBS in one pathway experiments (n=14)
-20180116 : began weak5Hz in one pathway experiments 
'''
import numpy as np
import scipy.io as io
from scipy import stats
from scipy import signal
import pandas as pd
import glob
import copy
import os
import matplotlib.pyplot as plt
import pickle
import itertools
import tkSimpleDialog as tk
from datetime import datetime
import pdb

class PreFuncs:
    ''' functions to organize raw traces, apply filters and get experimental details from comments
    '''

    def __init__(self, ):
        '''
        '''
        pass
        # self._load_and_organize(directory=directory, file_name=file_name)

    def _reorganize_comments(self, com, comtext, single_path=False, total_blocks=0):
        ''' 
        ==Args==
        -com : comment info np array organized as comments x info type (exported from LabChart)
                -column 0: channel, 
                -column 1: block, 
                -column 2:tick position, 
                -column 3: comment type, 
                -column 4: comtext map
        -comtext : np array of comments (exported from LabChart)
                -spaces are used to fill each comment so that they are all the same length use comtext[i].split()[0] to remove spaces from ith comment
        ==Out==
        -comment_dict: new comment structure organized as comment_dict{comment}{comment info type}[info]
                -comment info types: 'channel','block','tick_postiion','type','comment'

        -comment_list : list of all comment instances, with each instance stored as a tuple: (channel, block, tick_postition, type, comment) 
        ==Updates==
        ==Comments==
        -note that original data as exported from LabChart is in matlab format and has indexing starting from 1.  this function resets all indexing to start from 0 to be consistent with python
        '''
        # print single_path
        print 'reorganizing labchart comments'
        # dictionary to hold comments
        comment_dict={}
        # list of comments with tuples (channel, block, tick_position, type, comment)
        comment_list=[]

        comtext = np.array(comtext, dtype=object)
        # if single path, add 'path1' comment to each block 
        if single_path:
            for comment_i, comment_long  in enumerate(comtext):
                comment = comment_long.split()[0]
                if 'Apical' in comment:
                    comtext[comment_i] = 'apical'
                if 'Soma' in comment:
                    comtext[comment_i] = 'soma'
                if 'Basal' in comment:
                    comtext[comment_i] = 'basal'
                if 'Perforant' in comment:
                    comtext[comment_i]= 'perforant'

                # print comment
                if 'induction' in comment:
                    # print 'induction found'
                    # print comtext[comment_i], comment+'_path1'
                    comtext[comment_i] = comment+np.string_('_path1')
                    # print comtext[1]
            # print comtext
            # add comment to comtext
            n_comments = comtext.shape[0]
            comtext = np.append(comtext, 'path1')
            # print len(comtext)
            pathcom_i=n_comments+1
            # print pathcom_i

            # get all blocks
            all_blocks=range(1,total_blocks+1)
            # all_blocks = list(set(com[:,1]))
            # print all_blocks

            new_com = np.zeros((len(all_blocks), 5))
            # iterate over blocks
            for block_i, block in enumerate(all_blocks):
                new_com[block_i, 0] = 1
                new_com[block_i, 1] = block
                new_com[block_i, 2] = 1
                new_com[block_i, 3] = 1
                new_com[block_i, 4] = pathcom_i

            com = np.append(com, new_com, axis=0)

            # com_is = np.where(new_com[:,4]==pathcom_i)[0]
            # print com_is
            #FIXME add path to induction comments!!!!
        # print new_com[:,4].shape
        # print comtext
        # iterate over all comments
        for comment_i, comment_long in enumerate(comtext):
            # FIXME hack to remove accidental dashes instead of underscore
            comment_long = comment_long.replace('-','_')
            # remove spaces from comment
            comment = comment_long.split()[0]
            # print comment_i, comment
            # get all indices of current comment in com info array
            com_is = np.where(com[:,4]==comment_i+1)[0]
            # store list of information associated with each comment
            comment_dict[comment] = {
            'channel':com[com_is,0]-1,
            'block':com[com_is,1]-1,
            'tick_position':com[com_is,2]-1,
            'type':com[com_is,3]-1,
            'comment': [comment for temp in com_is]
            }

            # convert all info values to integers
            for key, val in comment_dict[comment].iteritems():
                
                if key!='comment':
                    comment_dict[comment][key] = [int(temp) for temp in val]

            # zip comments to store in list
            comment_zip = zip(comment_dict[comment]['channel'],
                comment_dict[comment]['block'],
                comment_dict[comment]['tick_position'],                comment_dict[comment]['type'],
                comment_dict[comment]['comment'])

            # add to comment list
            comment_list+=comment_zip

        # sort comments according to block number
        comment_zip.sort(key=lambda x:x[1])

        # print comment_dict['path1']
        # comments{specific comment}{info type}[info list]
        return comment_dict, comment_list

    def _get_baseline_percent(self, comment_dict, default_percent=0):
        '''find the comments that specifiy the percent of maximum that baseline epsps are set to. to be applied to each row of the group dataframe
        '''
        percent={'path1':[],'path2':[]}
        baseline_percent={'path1':[],'path2':[]}
        # check if there are any comments indicating the baseline percent of max
        if not any(['percent' in com and 'baseline' in com for com in comment_dict.keys()]):

            baseline_percent = {'path1':default_percent, 'path2':default_percent}

        else:
            # get comment strings that indicate the baseline percent
            percent_strings = [temp for temp in comment_dict.keys() if 'percent' in temp and 'baseline' in temp]
            # get the actual percent values from the comment strings
            percents = [float(percent_string.split('percent')[0][-2:]) for percent_string in percent_strings]

            # iterate through comment strings and find the one that corresponds to the current pathway
            # percents=[]
            for string_i, percent_string in enumerate(percent_strings):

                # get the block that the comment was made on 
                block = comment_dict[percent_string]['block'][-1]

                # determine which pathway the comment belongs to
                if block in comment_dict['path1']['block']:
                    path = 'path1'
                elif block in comment_dict['path2']['block']:
                    path='path2'
                else:
                    print 'path not found'
                    path='none'

                percent[path].append((block, percents[string_i]))

            # n_percents = list(set([path_temp[1] for path_temp_key, path_temp in percent.iteritems() ]))
            if len(list(set(percents)))==1:
                for path in percent:
                    baseline_percent[path]=list(set(percents))[0]
            else:

                for path in percent:
                    if percent[path]:
                        baseline_percent[path] = sorted(baseline_percent[path], key=lambda tup:tup[0])[-1][1]

        return baseline_percent
    
    def _get_baseline_max_idx(self, comment_dict, path_blocks, default_index=0):
        '''find the comments that specifiy the percent of maximum that baseline epsps are set to. to be applied to each row of the group dataframe
        '''
        default_index=np.nan
        # baseline_max={'path1':[],'path2':[]}
        baseline_max_block={'path1':[],'path2':[]}
        baseline_max_idx={'path1':[],'path2':[]}
        # check if there are any comments indicating the baseline percent of max
        # print comment_dict
        # print any(['max' in com and 'baseline' in com for com in comment_dict.keys()])
        if not any(['max' in com and 'baseline' in com for com in comment_dict.keys()]):

            baseline_max_block = {'path1':default_index, 'path2':default_index}
            baseline_max_idx = {'path1':default_index, 'path2':default_index}

        else:
            # get comment strings that indicate the baseline percent
            max_strings = [temp for temp in comment_dict.keys() if 'max' in temp and 'baseline' in temp]
            # get the actual percent values from the comment strings
            # max_vals = [float(max_string.split('_')[-1]) for max_string in max_strings]

            # iterate through comment strings and find the one that corresponds to the current pathway
            # percents=[]
            for string_i, max_string in enumerate(max_strings):
                for i, s in enumerate(comment_dict[max_string]['comment']):

                    # get the block that the comment was made on 
                    block = comment_dict[max_string]['block'][i]

                    # determine which pathway the comment belongs to
                    if block in comment_dict['path1']['block']:
                        path = 'path1'
                    elif block in comment_dict['path2']['block']:
                        path='path2'
                    else:
                        print 'path not found'
                        path='none'

                    baseline_max_block[path].append((block, max_string))

            # print baseline_max_block
            for path in baseline_max_block:
                if len(baseline_max_block[path])>0:
                    baseline_max_block[path] = sorted(baseline_max_block[path], key=lambda tup:tup[0])[-1][0]
                    baseline_max_idx[path] = path_blocks[path].index(baseline_max_block[path])
                else:
                    baseline_max_block[path]=np.nan
                    baseline_max_idx[path]=np.nan

            # n_percents = list(set([path_temp[1] for path_temp_key, path_temp in percent.iteritems() ]))
            # if len(list(set(max_vals)))==1:
            #     for path in baseline_max:
            #         baseline_percent[path]=list(set(percents))[0]
            # else:

            #     for path in percent:
            #         if percent[path]:
            #             baseline_percent[path] = sorted(baseline_percent[path], key=lambda tup:tup[0])[-1][1]

        return baseline_max_idx

    def _get_slice_info(self, comment_dict, filename, name, date, path_blocks):
        '''
        '''
        slice_info = {}
        # age

        slice_info['age'] = [float(com.split('age')[1].split('_')[1])for com in comment_dict.keys() if 'age' in com]
        slice_info['height'] = [float(com.split('height')[1].split('_')[1])for com in comment_dict.keys() if 'height' in com]
        slice_info['hemi'] = [com.split('hemi')[1].split('_')[1]for com in comment_dict.keys() if 'hemi' in com]

        slice_info['baseline_percent'] = self._get_baseline_percent(comment_dict)

        # pdb.set_trace()
        slice_info['baseline_max_idx'] = self._get_baseline_max_idx(comment_dict, path_blocks)
        print slice_info['baseline_max_idx'] 

        slice_info['filename']=filename
        slice_info['date']=date
        slice_info['name']=name

        return slice_info
        
    def _get_channel_locations(self, comment_dict, locations):
        '''
        ==Args==
        -comment_dict : comment dictionary as output from _reorganize_comments().
            ~organized as commnt_dic[comment][info type][instances]
                ~info types : 'channel','block','tick_postiion','type','comment'
        -locations : list of possible recording electrode locations (e.g. 'apical', 'soma', 'basal')
        ==Out==
        -channel_locations : list of (location, channel) pairs, e.g. [('apical',0), ('soma',1)]
        ==Updates==
        ==Comments==
        '''

        channel_locations =[]
        # iterate over possible electrode locations
        for location in locations:
            if location in comment_dict:
                # get channel number from comment dictionary
                # this assumes that a given recording channel corresponds to the same location throughout recording 
                channel = comment_dict[location]['channel'][0]
                channel_locations.append((location, channel))

        # list of tuples as [(location, channel)], e.g. [('soma',0),('apical',1)]
        return channel_locations

    def _get_path_blocks(self, comment_dict, paths, default_path='path1', total_blocks=0):
        '''
        ==Args==
        -comment_dict : comment dictionary as output from _reorganize_comments().
            ~organized as commnt_dic[comment][info type][instances]
                ~info types : 'channel','block','tick_postiion','type','comment'
        -paths : list of possible path names (e.g. 'path1', 'path2')
        ==Out==
        -path_blocks : organized as path_blocks[path name][block numbers]
        ==Updates==
        ==Comments==
        '''
        path_blocks = {}
        # iterate over possible path names
        for path in paths:
            if path in comment_dict:
                path_blocks[path] = comment_dict[path]['block']

        if len(path_blocks.keys())==0:
            path_blocks[default_path]=range(total_blocks)


        return path_blocks

    def _get_induction_blocks(self, comment_dict, keyword='induction'):
        '''
        ==Args==
        -comment_dict: comment dictionary as output from _reorganize_comments().
            ~organized as commnt_dic[comment][info type][instances]
                ~info types : 'channel','block','tick_postiion','type','comment'
        -keyword : string used to specify that the current block contains an induction protocol. typically 'induction'
        ==Out==
        -induction_blocks :  list of inductions organized as induction_blocks[induction number](induction comment, block number)
        ==Updates==
        ==Comments==
        -induction blocks can contain multiple entries from the same induction block, for example if there two pathways, there will be an entry for pathway in each induction block
        '''
        induction_blocks=[]
        # iterate over all commments
        for comment_key, comment in comment_dict.iteritems():
            # if comment is an induction comment
            if keyword in comment_key:
                # add to list
                for block in comment['block']:
                    induction_blocks.append((comment_key,block))

        # list of tuples as [(induction comment, block)], e.g. [('soma',0),('apical',1)]
        return induction_blocks

    def _get_induction_index(self, path_blocks, induction_blocks):
        '''
        '''
        induction_index = {}
        induction_blocks = list(set(zip(*induction_blocks)[1]))
        for path, blocks in path_blocks.iteritems():
            for induction_block in induction_blocks:
                blocks.append(induction_block)
                blocks.sort()
            induction_index[path] = [index for index, block in enumerate(blocks) if block in induction_blocks]

        return induction_index

    def _shift_induction_blocks_by_path(self, preprocessed):
        ''' FIXME add docs
        '''
        pre = preprocessed
        ind_idx={}
        for path in pre['path_blocks']:
            # all induction blocks
            ind_is = pre['data_induction']['apical']['blocks']
            # all blocks for current path or induction
            idx = [(temp_i,temp) for temp_i
            , temp in enumerate(pre['data_probe']['apical']['comments']) if path in temp or temp_i in ind_is]
            # induction index relative to the current path only 
            ind_idx[path] = [ (temp_i,temp[1]) for temp_i, temp in enumerate(idx) if temp[0] in ind_is]

        return ind_idx

    def _get_induction_info(self, induction_blocks, comment_dict,
        protocols = ['TBS','weak5Hz','15pulse5Hz','nostim'], 
        polarities=['anodal','cathodal','control'], 
        locations =['apical','basal','perforant'],
        drugs=[]):
        ''' extract induction parameters from comments in the induction block
        ==Args==
        -induction_blocks :  list of inductions organized as induction_blocks[induction number](induction comment, block number)
        -comment_dict : comment dictionary as output from _reorganize_comments().
            ~organized as commnt_dic[comment][info type][instances]
                ~info types : 'channel','block','tick_postiion','type','comment'
        -protocols : list of induction protocols to search for in comments
        -polarities : list of electric field polarities to search for in induction comments
        -locations : list of possible stimulating electrode locations
        -drugs : list of possible drugs used
        ==Out==
        -induction_info : induction_info[induction number]{info type/path}{path specific info}
            ~info types: 'field_magnitude', 'polarity', 'drugs', 'induction_num', 'induction_block', 'field_current', 'isolator_state', 'path1', 'path2'
            ~path specific info : 'location', 'protocol'
                ~e.g. induction_info[0]['path1']['location']='apical'
                ~e.g. induction_info[0]['path1']['protocol']='TBS'
        ==Updates==
        ==Comments==
        -induction blocks can contain multiple entries from the same induction block, for example if there two pathways, there will be an entry for pathway in each induction block
        -for a given induction, location in induction_info refers to the location of the stimulating electrode.  the location of the recording electrode is specified by the channel_locations attribute
        '''
        # get unique induction blocks
        induction_blocks_unique = sorted(list( set( [block for comment_key, block in induction_blocks])))
        # list containing a dictionary for each induction
        induction_info = []
        # iterate over unique induction blocks
        for ind_i, ind in enumerate(induction_blocks_unique):
            # dictionary for the current induction block
            ind_dict = {}
            # get a list of all comments in the current induction block
            ind_comments = [comment_key for comment_key, comment in comment_dict.iteritems() if ind in comment['block']]
            # print ind_comments
            # iterate over comments
            for comment in ind_comments:
                # if an induction comment
                if 'induction' in comment: 
                    # induction comments should be of the form 'induction_path1_TBS_anodal_20Vm_apical'.  induction info for each pathway can be extracted by splitting the induction comment along the underscores '_'

                    # split induction comment into subcomments
                    subcomments = comment.split('_')
                    print subcomments
                    # get electric field intensity
                    ind_dict['field_magnitude'] = [temp.lower().split('vm')[0] for temp in subcomments if 'Vm'.lower() in temp.lower()][0]
                    # print subcomments
                    # get electric field polarity
                    
                    ind_dict['polarity'] = [temp for temp in subcomments if temp in polarities][0]
                    # get drugs if any are used
                    if len(drugs)>0:
                        ind_dict['drugs'] = [temp for temp in subcomments if temp in drugs][0]
                    # get induction number
                    ind_dict['induction_num'] = [temp.split('induction')[-1] for temp in subcomments if 'induction' in temp][0]
                    # if no induction number is given, it is by default the first induction
                    if ind_dict['induction_num'] =='':
                        ind_dict['induction_num']='1'
                    # get induction block
                    ind_dict['induction_block']=ind
                    
                    # the path that the current induction comment describes
                    path_temp = [temp for temp in subcomments if 'path' in temp][0]
                    # the bipolar stimulator location for the current path
                    try:
                        location = [temp for temp in subcomments if temp in locations][0] 
                    except IndexError:
                        # if not induction location is given, assume it is apical
                        location = 'apical'

                    # induction protocol for the current path
                    protocol = [temp for temp in subcomments if temp in protocols][0]
                    # changed the name of weak 5 Hz stimulation 
                    if protocol=='15pulse5Hz':
                        protocol='weak5Hz'
                    # some slices have no comment for no bipolar stimulus during induction
                    if len(protocol)==0:
                        protocol='nostim'
                    # add protocol in each pathway to ind_dict
                    ind_dict[path_temp]=  {'location':location, 'protocol':protocol}

                elif 'field' in comment:
                    ind_dict['field_current'] = comment.split('_')[-1]

                elif 'isolator' in comment:
                    ind_dict['isolator_state'] = comment.split('_')[-1]

            induction_info.append(ind_dict)

        return induction_info

    def _get_induction_pulse_times(self, data_induction, induction_info, path_blocks, date,):
        '''
        ==Args==
        -data_induction : voltage data during induction blocks. organized as data_induction[channel][data type][induction number]
            ~data types : 'data', 'blocks', 'comments', 'fs'
            ~~data_probe['apical']['data'][induction number] = 1d array of voltage data
        -induction_info : induction_info[induction number]{info type/path}{path specific info}
            ~info types: 'field_magnitude', 'polarity', 'drugs', 'induction_num', 'induction_block', 'field_current', 'isolator_state', 'path1', 'path2'
            ~path specific info : 'location', 'protocol'
                ~e.g. induction_info[0]['path1']['location']='apical'
                ~e.g. induction_info[0]['path1']['protocol']='TBS'
        ==Out==
        -self.induction_info 
        ==Updates==
        -self.induction_info[induction_i][path]['input times']
        -self.induction_info[induction_i][path]['input index']
            ~for each induction number, for each path, 'input_times' and 'input_index' keys are added, with each being a list of bipolar input times/sample indices for the corresponding path during induction
        -self.induction_info[induction_i][path]['input_params'] : dictionary specifying pulse parameters for induction protocol
            ~params : 'pulse_freq'(Hz), 'burst_freq'(Hz), 'pulses' (per burst), 'bursts', 'onset' (s) 
        ==Comments==
        '''
        # name = self.file_name.split('\\')[-1]
        # date = int(name[:8])
        print data_induction.keys()
        channel=data_induction.keys()[0]
        path = data_induction[channel].keys()[0]

        fs = data_induction[channel][path]['fs'][0]
        input_params={
        'TBS':{
            'pulse_freq':100,
            'burst_freq':5,
            'pulses':4,
            'bursts':15,
            'onset':6
            },
        'weak5Hz':{
            'pulse_freq':100,
            'burst_freq':5,
            'pulses':1,
            'bursts':15,
            'onset':6.01
            },
        'nostim':{},
        }

        # if onset:
        #     input_params['TBS']['onset']=onset

        if date>= 20180613:
            input_params['TBS']['onset'] = 6
        else: 
            input_params['TBS']['onset']=2
        if date>=20180614:
            input_params['weak5Hz']['onset'] = 6.01
        elif date>=20180613:
            input_params['weak5Hz']['onset'] = 6.015
        elif date<20180613:
            input_params['weak5Hz']['onset'] = 2.015
        # iterate over channels 
        for channel_name, channel in data_induction.iteritems():
            for path, info in channel.iteritems():
                #iterate over inductions
                for induction_i, block in enumerate(info['blocks']):

                    # get induction protocol for current path
                    protocol = induction_info[induction_i][path]['protocol']
                    
                    # generate list of input times and corresponding sample indices for current path
                    # keep track of the pulse number 
                    cnt=-1
                    # list of input times
                    input_times=[]
                    # list of sample indices for corresponding input times
                    input_index=[]

                    if protocol!='nostim':
                        # iterate over bursts
                        for burst in range(input_params[protocol]['bursts']):
                            # iterate over pulse in current burst
                            for pulse in range(input_params[protocol]['pulses']):
                                cnt+=1
                                # get the time of the current pulse
                                time = input_params[protocol]['onset'] + float(burst)/input_params[protocol]['burst_freq'] + float(pulse)/input_params[protocol]['pulse_freq']
                                # add to main list
                                input_times.append(time)
                                input_index.append(int(time*fs))
                        # store in induction_info
                    induction_info[induction_i][path]['input_times'] = input_times
                    induction_info[induction_i][path]['input_index'] = input_index
                    induction_info[induction_i][path]['input_params'] = input_params[protocol]

        return induction_info

        # FIXME
        # if date >= 20180613, induction onset is at 6 s mark.  before 20180613, induction onset is at 2 s
        # if date >=20180614, onset of weak5Hz protocol is 6.01 s. before 20180614 weak5Hz onset is at either 2.015 or 6.015 s

    def _sort_induction_data_by_pulse(self, data_induction, induction_info, path_blocks):
        ''' reorganize induction data into a matrix with dimensions pulses x time
        ==Args==
        -data_induction : voltage data during induction blocks. organized as data_induction[channel][data type][induction number]
            ~data types : 'data', 'blocks', 'comments', 'fs'
            ~~data_probe['apical']['data'][induction number] = 1d array of voltage data
        -induction_info : induction_info[induction number]{info type/path}{path specific info}
            ~info types: 'field_magnitude', 'polarity', 'drugs', 'induction_num', 'induction_block', 'field_current', 'isolator_state', 'path1', 'path2'
            ~path specific info : 'location', 'protocol', 'input_times', 'input_index', 'input_params'
                ~e.g. induction_info[0]['path1']['location']='apical'
                ~e.g. induction_info[0]['path1']['protocol']='TBS'
        ==Out==
        -data_induction : added info type 'data_sortby_pulse'
            ~data_induction[channel name][induction number]['data_sortby_pulse'][path name] = numpy array with dimensions samples x pulse number
        ==Updates==
        -data_induction : added info type 'data_sortby_pulse'
            ~data_induction[channel name][induction number]['data_sortby_pulse'][path name] = numpy array with dimensions samples x pulse number
        ==Comments==
        '''
        channel=data_induction.keys()[0]
        path = data_induction[channel].keys()[0]
        #sampling rate
        fs = data_induction[channel][path]['fs'][0]
        # duration of each pulse in samples
        pulse_dur = int(.01*fs)
        # iterate over channels
        for channel_key, channel in data_induction.iteritems():
            for path, info in channel.iteritems():
                for data_type in info.keys():
                    if 'data' in data_type:
                        data_induction[channel_key][path][data_type+'_sortby_pulse']=[]
                        data_induction[channel_key][path][data_type+'_sortby_burst']=[]
                        # iterate over inductions
                        for induction_i, induction in enumerate(info[data_type]):
                            # list of input indices in samples
                            input_index = induction_info[induction_i][path]['input_index']
                            # print input_index
                            # bursts = induction_info[induction_i][path]['input_params']['bursts']
                            # preallocate sorted induction data
                            sorted_induction_data = np.zeros((pulse_dur, len(input_index)))
                            # sorted_induction_data = np.zeros((pulse_dur, len(input_index)))
                            # iterate over input pulses
                            if 'hilbert' in data_type:
                                sorted_induction_data = sorted_induction_data.astype('complex')

                            for pulse_i, pulse in enumerate(input_index):
                                pulse = int(pulse)

                                # print induction[pulse:pulse+pulse_dur].shape
                                # print sorted_induction_data.shape
                                # print pulse_i

                                sorted_induction_data[:,pulse_i]= induction[pulse:pulse+pulse_dur]
                                # print sorted_induction_data[:,pulse_i]



                            if 'bursts' in induction_info[induction_i][path]['input_params']:
                                bursts = induction_info[induction_i][path]['input_params']['bursts']
                                sorted_induction_data_byburst = sorted_induction_data.reshape((-1, bursts), order='F')
                            else:
                                sorted_induction_data_byburst = np.array([])
                            data_induction[channel_key][path][data_type+'_sortby_pulse'].append(sorted_induction_data)
                            data_induction[channel_key][path][data_type+'_sortby_burst'].append(sorted_induction_data_byburst)

        return data_induction

    def _filter_raw_data(self, data_probe, data_induction, filters, hilbert_filters=[]):
        '''
        ==Args==
        -data_probe : voltage data during baseline probes. organized as data_probe[channel][data type]
            ~data types: 'data', 'blocks', 'comments', 'fs'
            ~data_probe['apical']['data'] = array of voltage data with dimensions samples x blocks.  blocks where there is no probe (e.g. during induction) have a placeholder column of all zeros
        -data_induction : voltage data during induction blocks. organized as data_induction[channel][data type][induction number]
            ~data types : 'data', 'blocks', 'comments', 'fs'
            ~~data_probe['apical']['data'][induction number] = 1d array of voltage data
        -filters : dictionary as filters[filter key][b coefficients, a coefficients]
        ==Out==
        -data_probe, data_induction
        ==Updates==
        -filtered data are added to data_probe and data_induction as data_probe[channel key]['data_filt_filtername'][samples x blocks]
        data_induction[channel key]['data_filt_filtername'][induction number][samples]
        -filter coefficients are stored as data_probe[channel key]['filters'][b coefficients, a coefficients]
        ==Comments==
        '''
        # iterate over channels in recorded data
        for channel_key, channel in data_probe.iteritems():
            # iterate over paths
            for path, info in channel.iteritems():
                data_probe[channel_key][path]['filters']={}
                data_induction[channel_key][path]['filters']={}
                # iterate over filters
                for filt_key, filt in filters.iteritems():
                    # apply filter to corresponding probe data
                    probe_filtered = signal.filtfilt(filt[0],filt[1],info['data'], axis=0)
                    # store filtered data
                    data_probe[channel_key][path]['data_filt_'+filt_key]=probe_filtered
                    # store filters
                    data_probe[channel_key][path]['filters'][filt_key]=filt

                    # if filt_key in hilbert_filters:
                    #     probe_filtered_hilbert = np.abs(signal.hilbert(probe_filtered, axis=0))
                    #     data_probe[channel_key][path]['data_filt_'+filt_key+'_hilbert'] = probe_filtered_hilbert

                    data_induction[channel_key][path]['filters'][filt_key]=filt
                    data_induction[channel_key][path]['data_filt_'+filt_key]=[]
                    # if filt_key in hilbert_filters:
                    #     data_induction[channel_key][path]['data_filt_'+filt_key+'_hilbert']=[]
                    # iterate over inductions
                    for induction_num, induction in enumerate(data_induction[channel_key][path]['data']):

                        # filter the current induction data
                        induction_filtered = signal.filtfilt(filt[0],filt[1],induction)
                        # store filtered data
                        data_induction[channel_key][path]['data_filt_'+filt_key].append(induction_filtered)

                        # if filt_key in hilbert_filters:
                        #     induction_filtered_hilbert = np.abs(signal.hilbert(induction_filtered, axis=0))
                        #     data_induction[channel_key][path]['data_filt_'+filt_key+'_hilbert'].append(induction_filtered_hilbert)

        return data_probe, data_induction

    def _filter_raw_data_cascade(self, data_probe, data_induction, filters_series,):
        '''
        ==Args==
        -data_probe : voltage data during baseline probes. organized as data_probe[channel][data type]
            ~data types: 'data', 'blocks', 'comments', 'fs'
            ~data_probe['apical']['data'] = array of voltage data with dimensions samples x blocks.  blocks where there is no probe (e.g. during induction) have a placeholder column of all zeros
        -data_induction : voltage data during induction blocks. organized as data_induction[channel][data type][induction number]
            ~data types : 'data', 'blocks', 'comments', 'fs'
            ~~data_probe['apical']['data'][induction number] = 1d array of voltage data
        -filters : dictionary as filters[filter key][b coefficients, a coefficients]
        ==Out==
        -data_probe, data_induction
        ==Updates==
        -filtered data are added to data_probe and data_induction as data_probe[channel key]['data_filt_filtername'][samples x blocks]
        data_induction[channel key]['data_filt_filtername'][induction number][samples]
        -filter coefficients are stored as data_probe[channel key]['filters'][b coefficients, a coefficients]
        ==Comments==
        '''
        # iterate over channels in recorded data
        for channel_key, channel in data_probe.iteritems():
            # iterate over paths
            for path, info in channel.iteritems():
                data_probe[channel_key][path]['filters_cascade']={}
                data_induction[channel_key][path]['filters_cascade']={}
                # iterate over filters
                for filt_key, filt in filters_series.iteritems():
                    data_probe[channel_key][path]['filters_cascade'][filt_key]={}
                    data_induction[channel_key][path]['filters_cascade'][filt_key]={}
                    probe_filtered = copy.deepcopy(info['data'])
                    for subfilt_key, subfilt in filt.iteritems():
                        probe_filtered = signal.filtfilt(subfilt[0], subfilt[1], probe_filtered, axis=0)
                        data_probe[channel_key][path]['filters_cascade'][filt_key][subfilt_key]=subfilt
                        data_induction[channel_key][path]['filters_cascade'][filt_key][subfilt_key]=subfilt

                    # store filtered data
                    data_probe[channel_key][path]['data_filt_'+filt_key]=probe_filtered
                    # hilbert
                    #--------------
                    # if filt_key in hilbert_filters:
                    #     probe_filtered_hilbert = np.abs(signal.hilbert(probe_filtered, axis=0))
                    #     data_probe[channel_key][path]['data_filt_'+filt_key+'_hilbert'] = probe_filtered_hilbert

                    data_induction[channel_key][path]['data_filt_'+filt_key]=[]
                    # hilbert
                    # if filt_key in hilbert_filters:
                    #     data_induction[channel_key][path]['data_filt_'+filt_key+'_hilbert']=[]
                    # iterate over inductions
                    for induction_num, induction in enumerate(data_induction[channel_key][path]['data']):

                        induction_filtered = copy.deepcopy(induction)

                        for subfilt_key, subfilt in filt.iteritems():
                            # filter the current induction data
                            induction_filtered = signal.filtfilt(subfilt[0],subfilt[1],induction_filtered)
                        # store filtered data
                        data_induction[channel_key][path]['data_filt_'+filt_key].append(induction_filtered)
                        # hilbert
                        # if filt_key in hilbert_filters:
                        #     induction_filtered_hilbert = np.abs(signal.hilbert(induction_filtered, axis=0))
                        #     data_induction[channel_key][path]['data_filt_'+filt_key+'_hilbert'].append(induction_filtered_hilbert)

        return data_probe, data_induction

        # load i

    def _get_hilbert(self, data_probe, data_induction, hilbert_filters):
        '''
        '''
        # iterate over channels in recorded data
        for channel_key, channel in data_probe.iteritems():
            # iterate over paths
            for path, info in channel.iteritems():
                # iterate over filters
                for filt_key in hilbert_filters:
                    # apply filter to corresponding probe data
                    probe_hilbert = np.abs(signal.hilbert(info['data_filt_'+filt_key] - np.mean(info['data_filt_'+filt_key], axis=0), axis=0))
                    # store filtered data
                    data_probe[channel_key][path]['data_filt_'+filt_key+'_hilbert']=probe_hilbert
                    data_induction[channel_key][path]['data_filt_'+filt_key+'_hilbert']=[]
                    # iterate over inductions
                    for induction_num, induction in enumerate(data_induction[channel_key][path]['data_filt_'+filt_key]):

                        # filter the current induction data
                        induction_hilbert = np.abs(signal.hilbert(induction-np.mean(induction)))
                        # store filtered data
                        data_induction[channel_key][path]['data_filt_'+filt_key+'_hilbert'].append(induction_hilbert)
        return data_probe, data_induction

    def _preprocess(self, dic, date, filename, name, filters={}, filters_series={}, crop_probe=[0.5,.55], remove_offset=True, **kwargs):
        '''
        ==Args==
        -dic : data dictionary as exported from labchart
        -crop_probe : window for cropping probe traces to include only portion containing epsp. [min (s), max (s)].  epsp onset is at 0.5 s and typically lasts ~20 ms
        -remove_offset : if true, subtracts the value of the first data point from the voltage time series
        ==Out==
        -data_probe : voltage data during baseline probes. organized as data_probe[channel][data type]
            ~data types: 'data', 'blocks', 'comments', 'fs'
            ~data_probe['apical']['data'] = array of voltage data with dimensions samples x blocks.  blocks where there is no probe (e.g. during induction) have a placeholder column of all zeros
        -data_induction : voltage data during induction blocks. organized as data_induction[channel][data type][induction number]
            ~data types : 'data', 'blocks', 'comments', 'fs'
            ~~data_probe['apical']['data'][induction number] = 1d array of voltage data
        -path_blocks : organized as path_blocks[path key][block number]
        -induction_blocks : organized as induction_blocks[induction number](induction comment, block number)
            ~induction comments should be of the form 'induction_path_stimProtocol_fieldPolarity_fieldMagnitude_location'
        -induction_info : induction_info[induction number]{info type/path}{path specific info}
            ~info types: 'field_magnitude', 'polarity', 'drugs', 'induction_num', 'induction_block', 'field_current', 'isolator_state', 'path1', 'path2'
            ~path specific info : 'location', 'protocol'
                ~e.g. induction_info[0]['path1']['location']='apical'
                ~e.g. induction_info[0]['path1']['protocol']='TBS'
        -comment_dict: new comment structure organized as comment_dict{comment}{comment info type}[info]
                -comment info types: 'channel','block','tick_postiion','type','comment'

        -comment_list : list of all comment instances, with each instance stored as a tuple: (channel, block, tick_postition, type, comment) 
        ==Comemnts==
        -note that all indices in original data start at 1 as in matlab
        '''
        # dic = io.loadmat(directory+file_name)
        # split data into pathways
        # print kwargs
        # get shape of probe data
        # duration of each block in samples [blocks]
        block_durs = dic['dataend'][0,:]-dic['datastart'][0,:]+1
        # get the duration of probe blocks (should be the mode)
        probe_dur = int(stats.mode(block_durs).mode)
        # total number of probe blocks
        probe_num = int(stats.mode(block_durs).count)
        # number of channels
        nchannels = dic['datastart'].shape[0]
        # total number of blocks
        total_blocks = dic['datastart'].shape[1]
        # reorganize comments: comments{comment}{info type}[list integer values]
        if 'single_path' in kwargs:
            comment_dict, comment_list = self._reorganize_comments(com=dic['com'], comtext=dic['comtext'], single_path=kwargs['single_path'], total_blocks=total_blocks)
        else:
            comment_dict, comment_list = self._reorganize_comments(com=dic['com'], comtext=dic['comtext'])
        # channel_locations[('location',channel(0 index))]
        locations = ['soma','apical','basal','perforant','Soma', 'Apical', 'Basal','Perforant']
        channel_locations = self._get_channel_locations(comment_dict=comment_dict, locations=locations)
        # path_blocks{path}[list of blocks (0 index)]
        paths = ['path1','path2']
        path_blocks = self._get_path_blocks(comment_dict=comment_dict, paths=paths)
        # induction_blocks[('induction comment string', zero index block number)]
        induction_blocks = self._get_induction_blocks(comment_dict=comment_dict, keyword='induction')
        # induction_index[induction number] index is relative to the blocks in each path, rather than all blocks
        ind_idx = self._get_induction_index(path_blocks=path_blocks, induction_blocks=induction_blocks)
        # induction_info
        induction_info = self._get_induction_info(
            induction_blocks=induction_blocks, 
            comment_dict=comment_dict,
            protocols = ['TBS','weak5Hz','15pulse5Hz','nostim'], 
            polarities=['anodal','cathodal','control'], 
            locations =['apical','basal',],
            drugs=[]
            )
        # organize original data time series
        data_probe = {}
        data_induction = {}
        # print path_blocks
        print channel_locations
        # iterate over channels
        for channel_name, channel_i in channel_locations:
            print channel_name
            data_probe[channel_name]={}
            data_induction[channel_name]={}
            # iterate over paths
            for path, blocks in path_blocks.iteritems():
                # probe data is an array samples x total blocks. includes entries non-probe (e.g. induction blocks) as all zeros
                data_probe[channel_name][path] = {
                'data':np.zeros((probe_dur, len(blocks))),
                'blocks':[],
                'comments':[],
                'fs':[], 
                'filters':filters,
                'ind_idx':ind_idx[path]}
                # induction data stores as a list of 1D arrays, since induction periods may be of different length
                data_induction[channel_name][path] = {
                'data':[],
                'blocks':[],
                'comments':[],
                'fs':[],
                'filters':filters
                }
                # iterate over all recording blocks
                for block_i, start in enumerate(dic['datastart'][channel_i,:]):
                    
                    # start and stop indices, fs for current block
                    start_i = int(start-1)
                    end_i  = int(dic['dataend'][channel_i,block_i])
                    fs = int(dic['samplerate'][channel_i,block_i])
                    comments = [temp[-1] for temp_i, temp in enumerate(comment_list) if temp[1]==block_i]
                    # if no comments, use 'None' as place holder
                    if len(comments)==0:
                        comments=['None']
                    # if current block belongs to current path, store corresponding data
                    if block_i in blocks:
                        # check if block is a probe block
                        if block_durs[block_i] == probe_dur:
                            data_probe[channel_name][path]['blocks'].append(block_i)
                            data_probe[channel_name][path]['comments'].append(comments)
                            data_probe[channel_name][path]['fs'].append(fs)
                            
                            # add probe block data
                            data_probe[channel_name][path]['data'][:,blocks.index(block_i)] = dic['data'][0,start_i:end_i]
                    # check if block is an induction block
                    if block_i in zip(*induction_blocks)[1]:
                        print 'induction block found',channel_name
                        data_induction[channel_name][path]['blocks'].append(block_i)
                        data_induction[channel_name][path]['comments'].append(comments)
                        data_induction[channel_name][path]['fs'].append(fs)
                        # add block to induction data
                        data_induction[channel_name][path]['data'].append(dic['data'][0,start_i:end_i])
                        # data_all_induction.append(dic['data'][0,start_i:end_i])
                # print data_probe[channel_name][path]['fs']
            # data_probe[channel_name]['data_sorted']={}
            # for path, blocks in path_blocks.iteritems():
            #     data_probe[channel_name]['data'][path]=data_all_probe[:,blocks]
            #     data_induction[channel_name]['data'][path]=data_all_induction
        
        # print data_induction.keys()
        # apply filters
        data_probe, data_induction = self._filter_raw_data(data_probe, data_induction, filters)

        # apply filters
        data_probe, data_induction = self._filter_raw_data_cascade(data_probe, data_induction, filters_series)

        # apply filters
        data_probe, data_induction = self._get_hilbert(data_probe, data_induction, hilbert_filters=kwargs['hilbert_filters'])

        # sort induction data by pulses
        induction_info = self._get_induction_pulse_times(data_induction, induction_info, path_blocks, date)
        
        # print induction_info
        # print path_blocks
        data_induction = self._sort_induction_data_by_pulse(data_induction=data_induction, induction_info=induction_info, path_blocks=path_blocks)
        # get slice info, e.g. age, hemisphere
        slice_info = self._get_slice_info(comment_dict=comment_dict, date=date, name=name, filename=filename, path_blocks=path_blocks)
        # crop probe data after filter
        if crop_probe:
            if date<20161014:
                crop_probe=[0., 0.05]
            for channel in data_probe:
                for path in data_probe[channel]:
                    fs = stats.mode(data_probe[channel][path]['fs']).mode[0]
                    crop0 = int(crop_probe[0]*fs)
                    crop1 = int(crop_probe[1]*fs)

                    for info_type in data_probe[channel][path].keys():
                        # if info_type=='data_sorted':
                        #     for path, data in data_probe[channel][info_type].iteritems():
                        #         data_probe[channel][info_type][path]=data_probe[channel][info_type][path][crop0:crop1,:]
                        if 'data' in info_type:
                            data_probe[channel][path][info_type]=data_probe[channel][path][info_type][crop0:crop1, :]

                    if remove_offset==True:
                        data_probe[channel][path]['data'] = data_probe[channel][path]['data']- data_probe[channel][path]['data'][0,:]

        preprocessed={
        'data_probe':data_probe,
        'data_induction':data_induction,
        'path_blocks':path_blocks,
        'induction_blocks':induction_blocks,
        'ind_idx':ind_idx,
        'induction_info':induction_info,
        'comment_dict':comment_dict,
        'comment_list':comment_list,
        'slice_info':slice_info
        }
        return preprocessed

class Filters:
    '''
    '''
    def __init__(self, fs):
        '''
        '''

        # 300-1000 Hz bandpass
        self.filters={}
        nqst = fs/2
        # print nyquist

        self.filters_series= {
        'iir_band_5_50':{},
        'iir_band_300_1000':{}
        }

        self.filters_series['iir_band_5_50']['high_5'] = signal.iirdesign(
            wp = 5./nqst,
            ws= 0.1/nqst,
            gpass=1,
            gstop=20,
            ftype='butter')

        self.filters_series['iir_band_5_50']['low_50'] = signal.iirdesign(
            wp = 50./nqst,
            ws= 100./nqst,
            gpass=1,
            gstop=20,
            ftype='butter')

        self.filters_series['iir_band_300_1000']['high_300'] = signal.iirdesign(
            wp = 300./nqst,
            ws= 200./nqst,
            gpass=1,
            gstop=20,
            ftype='butter')

        self.filters_series['iir_band_300_1000']['low_1000'] = signal.iirdesign(
            wp = 1000./nqst,
            ws= 1100./nqst,
            gpass=1,
            gstop=20,
            ftype='butter')

        # self.filters['iir_high_1000'] = signal.iirdesign(
        #     wp = 1000./nqst,
        #     ws = 600./nqst,
        #     gpass=1.,
        #     gstop=80.,)

        self.filters['iir_high_300'] = signal.iirdesign(
            wp = 300./nqst,
            ws = 200./nqst,
            gpass=1.,
            gstop=80.,)
        
        # self.filters['iir_highpass_1000'] = signal.butter(
        #     5,
        #     [1000./nqst],
        #     btype='high'
        #     )

        self.filters['iir_high_5'] = signal.iirdesign(
            wp = 5./nqst,
            ws= 0.1/nqst,
            gpass=1,
            gstop=20,
            ftype='butter')

    def _plot_freqz(b, a, fs):
        '''
        '''
        w, h = signal.freqz(b, a)
        plt.figure()
        plt.plot((fs*0.5/np.pi)*w, abs(h))
        plt.show(block=False)

def _preprocess_and_save_new_files(
    raw_directory='Raw Matlab Data/', 
    processed_directory='Preprocessed Data/', 
    raw_ext='.mat', 
    processed_ext='.pkl', 
    file_limit=[],
    run_all=False,
    **kwargs
    ):
    '''
    ==Args==
    -raw_directory : directory (including '/') containing raw matlab files as exported from LabChart (typically 'Raw Matlab Data/')
    -processed_directory : directory (including '/') containing processed pickle files (typically Preprocessed Data/)
    -raw_ext : file extension for raw data ('.mat')
    -processed_ext : file extension for processed data
    -file_limit : maximum number of files to process per function call. if empty list, there is no limit
    ==Out==
    -preprocessed : dictionary of most recent processed data file.  see PreFuncs._organize_individual_data for details of preprocessed format
    ==Updates==
    -new raw data files are processed and saved in the processed folder as a pickle file containing the preprocessed dictionary
    ==Comments==
    '''
    # print kwargs
    print 'processing new files'
    # get list of all data files
    raw_files = glob.glob(raw_directory+'*'+raw_ext+'*')
    raw_filenames = [file.split('\\')[-1].split('.')[0] for file in raw_files]
    # get list of all data files
    processed_files = glob.glob(processed_directory+'*'+processed_ext+'*')
    processed_filenames = [file.split('\\')[-1].split('.')[0] for file in processed_files]
    # new files
    new_files = [raw_files[file_i] for file_i, file in enumerate(raw_filenames) if file not in processed_filenames]
    new_filenames = [file.split('\\')[-1].split('.')[0] for file in new_files]

    if run_all:
        new_files = raw_files
        new_filenames=raw_filenames

    print 'total data files:', len(raw_files) 
    print 'new data fies:', len(new_files)

    # get filters
    filters = Filters(fs=10000).filters
    filters_series = Filters(fs=10000).filters_series
    # load preprocessing functions
    prefuncs = PreFuncs()
    # iterate over new data files
    for file_i, file in enumerate(new_files):
        # get file name and date
        name = file.split('\\')[-1].split('.mat')[0]
        date = int(name[:8])
        # check for file limit condition
        if file_limit and file_i>file_limit:
            continue
        # limit number of files processed at a time
        else:

            print 'current file:', name
            #load raw data 
            raw_data_dict = io.loadmat(file)
            # file name for processed file
            processed_file = processed_directory + new_filenames[file_i] + processed_ext
            # apply preprocessing
            preprocessed = prefuncs._preprocess(dic=raw_data_dict, filters=filters, filters_series=filters_series, date=date, filename=processed_file, name=name, **kwargs)
            
            # pdb.set_trace()
            # save processed file in processed folder
            with open(processed_file,'wb') as pklfile:
                pickle.dump(preprocessed, pklfile, protocol=pickle.HIGHEST_PROTOCOL)
            print 'saved processed file:', processed_file

kwargs={'hilbert_filters':['iir_high_300', 'iir_band_300_1000']}
# _preprocess_and_save_new_files(raw_directory='Raw Matlab Data 1Path/', processed_directory='Preprocessed Data 1Path/', run_all=False, file_limit=[], single_path=True, **kwargs)

# _preprocess_and_save_new_files(raw_directory='Raw Matlab Data/', processed_directory='Preprocessed Data/', run_all=False, file_limit=[], single_path=False, **kwargs)

_preprocess_and_save_new_files(raw_directory='Raw Matlab Data Mahima/', processed_directory='Preprocessed Data Mahima/', run_all=False, file_limit=2, single_path=True, **kwargs)

if __name__ =='__main__':
    _preprocess_and_save_new_files()
